# SQL Benchmark Toolkit

SQL Benchmark Toolkit provides a consolidated home for generating synthetic SQL data, parsing queries, and evaluating text-to-SQL systems. The project bundles data preparation utilities, generation pipelines, and evaluation helpers used in our experiments on the Spider benchmark and derivative datasets.

## Quick start

1. **Set up a Python environment** (Python 3.10+ is recommended):
   ```bash
   python -m venv .venv
   source .venv/bin/activate
   pip install -U pip
   ```
2. **Install the project in editable mode** so that scripts can rely on the new `src/` package layout:
   ```bash
   pip install -e .
   ```
3. **Explore the documentation** in [`docs/`](docs/README.md) for detailed walkthroughs of data preparation, generation pipelines, and evaluation tooling.

## Repository layout

| Path | Description |
| --- | --- |
| [`src/sql_benchmark`](src/sql_benchmark) | Installable Python package that contains the core modules for schema parsing, query generation, SQL→text utilities, and experiment analysis. |
| [`data/raw`](data/raw) | Canonical benchmark assets such as Spider table schemas and training splits. |
| [`data/processed`](data/processed) | Derived artefacts generated by synthesis and preprocessing steps. |
| [`data/archives`](data/archives) | Archived bundles (for example zipped snapshots of synthetic queries). |
| [`tools/scripts`](tools/scripts) | Shell helpers used to orchestrate large-scale synthesis and analysis runs. |
| [`tools/evaluation`](tools/evaluation) | Python helpers for assigning and running evaluation subsets. |
| [`outputs`](outputs) | Experiment outputs, logs, and generated figures. |
| [`artifacts/legacy`](artifacts/legacy) | Historical build artefacts retained for provenance. |
| [`docs`](docs) | Extended documentation covering project structure, workflows, and data contracts. |

A complete module-by-module map is available in [`docs/module-map.md`](docs/module-map.md).

## Working with the package

After running `pip install -e .`, the package exposes modules under the `sql_benchmark` namespace. For example, the Spider SQL parser can be imported with:

```python
from sql_benchmark.parser_sql.parse_sql_one import get_schema, get_sql
```

CLI-oriented utilities live under `tools/`. Each script contains usage instructions and expects the repository root to be the current working directory.

## Documentation highlights

- [`docs/getting-started.md`](docs/getting-started.md) – walkthrough covering environment setup, installing dependencies, and running the first synthetic query generation job.
- [`docs/data-guide.md`](docs/data-guide.md) – description of the raw and processed datasets bundled with the repository.
- [`docs/module-map.md`](docs/module-map.md) – per-module summary of the Python package layout.
- [`docs/workflows/evaluation.md`](docs/workflows/evaluation.md) – step-by-step instructions for reproducing evaluation runs with the reorganised tooling.

## Contributing

1. Open an issue describing proposed changes.
2. Fork the repository and create a feature branch.
3. Run formatters and relevant checks before submitting a pull request.

The new structure is designed to separate code, data, documentation, and experimental artefacts, making it easier for new contributors to understand where to add features or locate results.
