# Evaluation workflow

This document outlines how to run the reorganised evaluation helpers against prediction files.

## 1. Prepare the environment

Follow the instructions in [`docs/getting-started.md`](../getting-started.md) to install the package. Ensure the Spider evaluation toolkit is available under `test-suite-sql-eval-master` (bundled with the repository).

## 2. Collate evaluation subsets

The scripts in [`tools/evaluation`](../../tools/evaluation) generate per-database subsets used during benchmarking:

```bash
python tools/evaluation/assigning_evaluation_subsets.py --help
python tools/evaluation/evaluation_subsets.py --help
```

Both scripts assume that synthetic query CSV files reside under `data/processed/synthetic_queries/<method>/`.

## 3. Run end-to-end evaluations

Use the `sql_benchmark.analysis` package to call the Spider evaluation script and log outputs:

```bash
python -m sql_benchmark.analysis.analysis_res
```

Logs are saved to [`outputs/analysis`](../../outputs/analysis), one file per LLM/database combination. The helper uses `subprocess.run` and does not mutate predictions, making it safe to re-run.

Additional statistics (success rates, hardness distributions, generated charts) can be produced with:

```bash
python -m sql_benchmark.analysis.sql2text_analysis --synthesis_method schema_guided
python -m sql_benchmark.analysis.synthetic_statistics --synthesis_method llm_based --output_file outputs/experiments/SQL2text/llm_based/summary.json
```

Each module automatically creates any missing directories beneath `outputs/experiments`.

## 4. Interpreting results

- Check the CSV and text outputs stored under `outputs/experiments/...` for aggregated accuracy figures.
- Visual assets generated by the analysis scripts are written alongside the numeric summaries for easy embedding in reports.

Refer back to [`docs/module-map.md`](../module-map.md) if you need to dive into the implementation details of the analysis modules.
